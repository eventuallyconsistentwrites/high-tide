# Load Testing `high-tide-server` with K6

This project uses **Grafana K6** running in Docker to generate load on the demo server.

## Architecture

This uses a **Hybrid Scaling** approach to maximize efficiency while maintaining unique IP addresses:

* **Scaling Containers:** We scale Docker containers to provide distinct source IP addresses (bypassing rate limits).
* **K6 Internals:** Each container runs multiple lightweight "Virtual Users" (VUs) internally.

**Example:**
Running **10 containers** with **100 VUs** each generates **1,000 Concurrent Users** originating from **10 distinct IPs**.

---

## Running the Test

### 1. Basic Run (Test Logic)
To verify the script works with a single container (1 IP):

```bash
# also do `docker-compose build --no-cache high-tide-server` if you do not see code changes reflect.
docker-compose up k6-worker
```

### 2. Scaled Run (High Load, Multiple IPs)

To run a distributed test simulating traffic from multiple IP addresses, scale the `k6-worker` service:

```bash
# Starts 10 worker containers.
# If K6_VUS is set to 100, this generates 1,000 total concurrent users.
docker-compose up -d --scale k6-worker=10
```

> **Note:** You do not need the `--build` flag for the load tester anymore, as it pulls the official `grafana/k6` image and mounts your local script.

---

## Configuration

### Adjusting Load Intensity

You can control the number of users *per container* by setting the `K6_VUS` environment variable.

**Example: 5 containers x 50 users = 250 Total Users**

```bash
K6_VUS=50 docker-compose up --scale k6-worker=5
```

**Running for 5 minutes**

```bash
# Overriding the command to run for 5 minutes in detached mode
K6_VUS=50 K6_DURATION=5m docker-compose up -d --scale k6-worker=5
# Seeing logs
docker-compose logs -f k6-worker
```

### Modifying the Test Script

The test logic is defined in `loadtest.js`.

1. Edit `loadtest.js` locally (e.g., change the sleep duration or endpoint).
2. Restart the test. **No rebuild is required.**

---

## Monitoring

### View Logs

Watch the aggregate logs from all K6 workers to see the test progress:
```bash
docker-compose logs -f k6-worker
```

Similarly, to see logs of high-tide-server:
```bash
docker-compose logs -f high-tide-server
```

### Verify Source IPs

To confirm that your load is coming from different IP addresses, list the internal Docker IPs for all worker containers:

```bash
docker inspect --format '{{.Name}} - IP: {{.NetworkSettings.Networks.loadtester_default.IPAddress}}' $(docker-compose ps -q k6-worker)
```

---

## The Set-up Used

1. Build the high-tide-server with --no-cache to fetch latest code changes
```bash
docker-compose build --no-cache high-tide-server
```

2. Run one high-tide-server container and 100 k6-worker containers each having 1000 virtual users.
```bash
K6_VUS=1000 K6_DURATION=1m RL_MODE=cms CMS_CERTAINTY=0.001 CMS_ERROR_MARGIN=0.01 THRESHOLD=200 docker-compose up -d --scale k6-worker=100
# RL_MODE can take the values cms, map or none
```

3. Redirect the k6-worker logs and high-tide-server logs to log files
```bash
docker-compose logs k6-worker > k6-worker.log
docker-compose logs high-tide-server > high-tide-server.log
# This was done for each RL_MODE value
```

4. Visualise results using python script
```bash
python3 visualise-k6.py k6-worker.log
```

---

## Visualising using Logs

### K6 Log Visualizer

This utility script ([`/loadtester/visualise-k6.py`](https://github.com/eventuallyconsistentwrites/high-tide/blob/main/loadtester/visualise-k6.py)) parses `docker-compose` logs generated by K6 worker containers and visualizes the performance metrics. It is designed to compare different load testing scenarios (e.g., different rate-limiting algorithms like Count-Min Sketch vs. Hash Maps) by generating bar charts for **Throughput** and **Latency**.

#### Prerequisites

Ensure you have Python installed (3.6+ recommended). You also need the following libraries:
```bash
pip install matplotlib numpy
```

#### Usage

The script accepts one or more log files as command-line arguments. It parses each file, aggregates the metrics from all worker containers found within, and plots them side-by-side.

##### Basic Syntax

```bash
python visualize_k6.py <path_to_log_file_1> [<path_to_log_file_2> ...]
```

##### Examples

**1. Analyze a single run:**
View the aggregate performance of all workers in the CMS mode.
```bash
python visualize_k6.py k6-worker-cmsmode.log
```

**2. Compare multiple modes (Recommended):**
Compare the performance of CMS mode, Map mode, and No protection side-by-side.
```bash
python visualize_k6.py k6-worker-cmsmode.log k6-worker-mapmode.log k6-worker-nonemode.log
```

#### How It Works

1. **Parsing:** The script reads the raw text logs looking for specific K6 summary lines (output by `docker-compose`):
    * `http_reqs`: Extracts the Requests Per Second (RPS) and total count.
    * `http_req_duration`: Extracts Average, Median, and P95 latency values.
    * `http_req_failed`: Extracts the failure rate (useful for filtering out broken runs).

2. **Aggregation:** It sums up the Throughput (RPS) of all workers in a single file to get the **Total System Throughput**. It averages the latency metrics to get a global view of response times.
3. **Visualization:** It uses `matplotlib` to generate two charts:
    * **Total System Throughput (RPS):** Higher is better.
    * **Response Latency (ms):** Lower is better. Compares Average vs. P95.

#### Output Interpretation

When running the script, two graphs will appear in a popup window

##### 1. Total System Throughput (Left Chart)

* **Y-Axis:** Requests Per Second (RPS).
* **Meaning:** This represents the total load the server successfully handled (or rejected with a valid HTTP code) across all workers.
* **What to look for:** A higher bar indicates the server processed more traffic per second. If `cmsmode` is higher than `mapmode`, the CMS algorithm is more efficient.

##### 2. Response Latency (Right Chart)

* **Y-Axis:** Time in Milliseconds (ms).
* **Yellow Bar (Avg):** The average time a request took.
* **Red Bar (P95):** The 95th percentile (the time within which 95% of requests completed).
* **What to look for:** Lower bars are better. A large gap between the Yellow and Red bars indicates high variance (jitter) or occasional server stalls.

#### Troubleshooting

* **"No usable data found":** Ensure your log files contain the K6 summary block (e.g., `k6-worker-1 | http_reqs...`). If the workers crashed before printing the summary, the script cannot parse the data.
* **"dial: i/o timeout":** If you see this in your raw logs (common in `nonemode`), the script will likely report very low or zero RPS because the server was unresponsive.

### High-Tide Server Log Visualizer

Here is the documentation for the server-side log visualizer, following the exact style and structure of the K6 documentation you provided.

---

#### High-Tide Server Log Visualizer

This utility script (`visualise-server.py`) parses the backend logs generated by the `high-tide-server` container. Unlike the K6 visualizer (which shows what the *client sent*), this script visualizes what the *server actually processed*. It is essential for analyzing server stability, lock contention, and crash behavior under load.

##### Prerequisites

Ensure you have Python installed (3.6+ recommended). You also need the following libraries:

```bash
pip install matplotlib
```

##### Usage

The script accepts one or more server log files as command-line arguments. It filters the logs for rate-limit check events, calculates the Requests Per Second (RPS) over time, and plots the traffic flow.

###### Basic Syntax

```bash
python visualise-server.py <path_to_server_log_1> [<path_to_server_log_2> ...]
```

###### Examples

**1. Analyze a single server run:**
View the traffic stability of the Count-Min Sketch mode.

```bash
python visualise-server.py high-tide-server-cmsmode.log
```

**2. Compare server stability (Recommended):**
Overlay the performance of CMS mode, Map mode, and None mode to see the efficiency gap.

```bash
python visualise-server.py high-tide-server-cmsmode.log high-tide-server-mapmode.log high-tide-server-nonemode.log

```

##### How It Works

1. **Parsing:** The script reads raw log lines, extracting the JSON object embedded in the Docker output.
* It filters specifically for the log message: `"msg": "checking rate limit"`.
* This event represents a request that successfully reached the application layer and required processing.

2. **Time Series Calculation:** It extracts the ISO-8601 timestamps from the logs and buckets them into 1-second intervals to calculate the instantaneous RPS.
3. **Visualization:** It uses `matplotlib` to generate a **Line Chart**:
* It applies a slight "smoothing" (moving average of 3 seconds) to make the graph more readable.
* It calculates the average RPS over the duration of the test for the legend.

##### Output Interpretation

When running the script, a line graph will appear in a popup window.

###### Server-Side Traffic (Requests Processed/Checked)

* **X-Axis:** Time Elapsed (seconds).
* **Y-Axis:** Requests Per Second (RPS).
* **What to look for:**
* **The "CMS Mode" Line (High & Flat):**
* You should see a high, stable line (e.g., ~300 RPS).
* **Meaning:** The server is processing requests efficiently with minimal internal friction.

* **The "Map Mode" Line (Lower & Jagged):**
* This line will likely be lower than CMS (e.g., ~220 RPS).
* **Meaning:** The gap between this line and the CMS line represents **Lock Contention**. The server is spending CPU cycles waiting for locks on the Hash Map instead of processing requests.

* **The "None Mode" (Missing or Crash):**
* You will likely see no line, or a line that abruptly stops.
* **Meaning:** The server crashed or froze immediately due to resource exhaustion, stopping it from writing further logs.

##### Troubleshooting

* **"No request data found":** This is expected for `nonemode`. If the server becomes unresponsive (deadlock or OOM) immediately, it cannot write the "checking rate limit" log entry, resulting in an empty dataset.
* **Timezone issues:** The script attempts to parse standard ISO-8601 timestamps. If your docker logs are using a custom non-standard time format, the regex parser might skip those lines.

---